Problem Motivation: Real-world Justification and Significance

Real-world pathfinding is critical for applications like Google Maps, video games, and robotics. Traditional algorithms like A* ensure optimal paths but can be slow due to extensive node expansions. This inefficiency impacts:
1. **Video Games & Robotics**: Lag in AI characters or robots.
2. **Self-Driving Cars**: Delayed reactions to traffic changes.
3. **Warehouse Automation**: Slower package handling by robots.
4. **Emergency Response**: Delays in routing ambulances.

This project enhances pathfinding using Q-Learning (machine learning) to guide the search. By learning from experience, we aim to reduce computation by 30% while maintaining near-optimal paths (within 5-10%), balancing speed and accuracy for real-time efficiency.

================================================================================

Core Algorithms and Data Structures: Technique Description and Algorithmic Rationale

We implemented four main algorithms:
1. **A* Algorithm**: The baseline. Uses a priority queue and f(n) = g(n) + h(n) (Euclidean distance). Guarantees optimal paths but explores many nodes.
2. **Weighted A***: A faster, greedy variant where f(n) = g(n) + w * h(n) (w > 1). Explores fewer nodes but may produce sub-optimal paths.
3. **Q-Learning**: A reinforcement learning agent that learns a Q-table of state-action values through trial and error (500 episodes). It uses epsilon-greedy exploration and updates Q-values based on rewards (reaching the goal) and penalties (steps taken).
4. **Q*-A* (Hybrid Innovation)**: Combines A* with Q-Learning. The priority function is f'(n) = g(n) + h(n) - β * bias(n). bias(n) is derived from learned Q-values, guiding A* toward promising areas. β controls the influence of the learned bias.

Data Structures:
- **Graph (Adjacency List)**: Efficient O(degree) neighbor traversal.
- **Priority Queue (Min-Heap)**: O(log n) operations for A* node selection.
- **Q-Table (HashMap)**: O(1) lookup for learned values.
- **Hash Maps**: For g_cost, parent, and closed_set.

Complexity: Time: O(E log V). Space: O(V + E). Q*-A* reduces constant factors (expansions) without changing worst-case complexity.

================================================================================

Methodology: System Architecture and Experimental Design

System Architecture:
- **graph.py**: Graph class with adjacency list.
- **geometry.py**: Generates grid maps with random obstacles.
- **astar.py** & **qlearning.py**: Implementations of A* and Q-Learning.
- **qstar.py**: The hybrid Q*-A* algorithm.
- **experiments.py**: Runs benchmarks and collects metrics.
- **parallel_utils.py**: Uses multiprocessing for concurrent trials.

Experimental Design:
- **Environment**: 40x40 grid, 20% obstacle density, 8-directional movement.
- **Training**: 800 episodes, learning rate α=0.2, discount γ=0.95.
- **Testing**: 30 independent runs on random maps. Compares A* (optimal), Weighted A* (fast), and Q*-A*.

Metrics:
1. **Node Expansions**: Computational effort (lower is better).
2. **Runtime**: Execution time.
3. **Path Cost**: Length of the found path.
4. **Cost Ratio**: cost / optimal_cost (target ≤ 1.10).
5. **Expansion Reduction**: % reduction vs baseline (target ≥ 30%).

Success Criteria: Q*-A* should reduce expansions/runtime by 15-30% vs A* while keeping cost within 10-20% of optimal, outperforming Weighted A* in quality/efficiency balance.

================================================================================

Future Work: Directions for Optimization or Expansion

1. **Deep Q-Learning**: Replace Q-table with Neural Networks (DQN) to scale to large maps and generalize to unseen states.
2. **Dynamic Obstacles**: Adapt for moving obstacles or changing maps using incremental training or experience replay.
3. **Multi-Agent Pathfinding**: Coordinate multiple agents to avoid collisions in warehouses or traffic.
4. **Hyperparameter Tuning**: Use grid search or adaptive beta to optimize performance.
5. **Bidirectional Search**: Implement bidirectional Q*-A* to potentially halve search time.
6. **Alternative Heuristics**: Test Manhattan or landmark-based heuristics.
7. **Real-World Application**: Deploy on physical robots or in game engines (Unity/Unreal).
8. **Theoretical Analysis**: Formal proofs for bounds on suboptimality and complexity.
9. **Comparison with Modern Techniques**: Benchmark against JPS, Theta*, or D* Lite.
10. **Transfer Learning**: Train a universal agent on features (e.g., local density) rather than specific maps.
11. **Visualization Tools**: Heatmaps and animations to debug and present results.
12. **Energy-Aware Pathfinding**: Optimize for energy consumption on heterogeneous terrains.

Top Picks: Deep Q-Networks for scalability, Multi-Agent Pathfinding for complexity, and Real Robot Implementation for validation.
